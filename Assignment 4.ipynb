{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Summer School - Assignment 4\n",
    "\n",
    "In this assignment, we will be clustering movies on the basis of their synopsis. Unlike previous assignments, this time you will be implementing the KMeans clustering algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from sklearn import feature_extraction\n",
    "import pickle\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "We will be using the synopsis of Top 100 movies ranked by IMDb. `titles`, `genres` and `synopsis` are lists stored as pickled files. We first load them into memory. \n",
    "\n",
    "Take a look at the lists we have just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "titles = pickle.load(open('data/titles.pkl','rb'))\n",
    "genres = pickle.load(open('data/genres.pkl','rb'))\n",
    "synopsis = pickle.load(open('data/synopses.pkl','rb'))\n",
    "print(len(titles), len(genres), len(synopsis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at some of the synopsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(synopsis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(synopsis[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the synopsis contain a lot of names (Proper Nouns) and years. They are specific to the movies and do not seem to be helpful to cluster the movies. We should remove them\n",
    "\n",
    "We will also be lemmatizing the words. [Lemmatizing](https://www.twinword.com/blog/what-is-lemmatization/ ) refers to replacing word by its base form (lemma). For example, lemmatizing `cars` gives `car`. Lemmatizing will help in clubbing all different inflections of words, eg. `loves`, `loving`, `loved` will all be lemmatized to `love`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function `preprocess_data`. It takes in text as input, and [removes proper nouns](https://stackoverflow.com/questions/39634222/is-there-a-way-to-remove-proper-nouns-from-a-sentence-using-python), non-alphabetic words, lemmatizes the data and lower cases the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "st1=SnowballStemmer(\"english\")\n",
    "#st2= PorterStemmer(\"english\")\n",
    "\n",
    "def preprocess_data(text):\n",
    "    \"\"\"Removes proper noun, non-alphabetic words, lemmatizes the data and lower cases the entire text\"\"\"\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    tagged_sentence = nltk.tag.pos_tag(text.split()) \n",
    "    #print(tagged_sentence)\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    \n",
    "    #print(' '.join(edited_sentence))\n",
    "    text=' '.join(edited_sentence)\n",
    "   # print(text)\n",
    "   # print()\n",
    "    text=(' '.join(filter(lambda x: x not in stopwords.words(\"english\"),text.split())))\n",
    "#    print(text)\n",
    "\n",
    "    text=text.lower() \n",
    "    #re.sub(\"[^a-z]+\", '',text)\n",
    "\n",
    "    #text=text.replace(/[^a-zA-Z ]/g, \"\"));\n",
    "    text=re.sub('[^a-zA-Z]+', ' ',text)\n",
    "    #print(text)\n",
    "    #print()\n",
    "   # text=(list(filter(lambda x:stemming(x),text.split())))\n",
    "    #print(text)\n",
    "    #print()\n",
    "    \n",
    "   # text=(' '.join(text))\n",
    "    stemmer=SnowballStemmer(\"english\")\n",
    "    \n",
    "    text = \" \".join([ stemmer.stem(kw) for kw in text.split(\" \")])\n",
    "\n",
    "        \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample input**: preprocess_data(synopsis[0][:1000])\n",
    "\n",
    "\n",
    "**Sample output**: 'edit edit edit day daughter wedding hears request role crime family son uniform introduces girlfriend family reception godson singer pleads help securing movie role dispatch consigliere influence studio head is unmoved morning wake bed head stallion day daughter wedding hears request role crime family son uniform introduces girlfriend family reception godson'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit edit on day daughter s wed hear request role crime famili youngest son uniform introduc girlfriend famili sprawl recept godson popular singer plead help secur covet movi role dispatch consiglier influenc abras studio head unmov morn wake bed sever head prize stallion on day daughter s wed hear request role crime famili youngest son uniform introduc girlfriend famili sprawl recept godson\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(preprocess_data(synopsis[0][:1000]))\n",
    "#print(synopsis[0][:1000])\n",
    "#stemmer.stem(\"requests\")\n",
    "#print(preprocess_data(\"requests\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Features\n",
    "\n",
    "We will extract features from the synopsis using TfIdf Vectoriser, which will be used to cluster the movies. \n",
    "\n",
    "Read more about TfIdf Vectoriser [here](http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/) and [here](http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=20000, min_df=0.2,\\\n",
    "                                   stop_words='english', preprocessor=preprocess_data, \\\n",
    "                                   use_idf=True, ngram_range=(1, 1))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopsis)\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=20000, min_df=0.2,\\\n",
    "#                                   stop_words='english', preprocessor=preprocess_data, \\\n",
    "#                                   use_idf=True, ngram_range=(1, 1))\n",
    "\n",
    "#%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopsis)\n",
    "\n",
    "#print(tfidf_matrix.shape)\n",
    "#terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Implementing KMeans\n",
    "\n",
    "This is the most important task of this assignment because in this, you will implement your own K-Means. Fill in the function `KMeans` which will return the labels and cluster centers in the tuple `(labels,centers)` for the given feature matrix `X` and `num_clusters`. We also pass the `max_iter` parameter to run KMeans for that many iterations as it sometimes gets stuck on a local minima. You can learn more about Kmeans clustering [here](https://en.wikipedia.org/wiki/K-means_clustering) and [here](https://www.datascience.com/blog/k-means-clustering).\n",
    "\n",
    "**Sample Input**: `[[2,1], [2,3], [8,1], [8,3]]`, `num_clusters=2`\n",
    "\n",
    "**Sample Output**: `([0,0,1,1], [[2,2], [8,2]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def Kmeans(X, num_clusters=8, max_iter=300):\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "    centers={}\n",
    "    labels=[]\n",
    "    j=0\n",
    "    for i in random.sample(range(X.shape[0]),num_clusters):\n",
    "        centers[j] = X[i]\n",
    "        j=j+1\n",
    "    cs = {}\n",
    "    for i in range(max_iter):\n",
    "            cs = {}\n",
    "            labels=[]\n",
    "            for i in range(num_clusters):\n",
    "                cs[i] = []\n",
    "\n",
    "            for featureset in X:\n",
    "                dist = [np.linalg.norm(featureset-centers[centroid]) for centroid in centers]\n",
    "                cn = dist.index(min(dist))\n",
    "                cs[cn].append(featureset)\n",
    "                labels.append(cn)\n",
    "           \n",
    "\n",
    "            for cn in cs:\n",
    "                centers[cn] = np.average(cs[cn],axis=0)\n",
    "   \n",
    "    labels = np.asarray(labels)\n",
    "    centers = np.asarray(list(centers.values()))\n",
    "                \n",
    "\n",
    "         \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return (labels, centers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "#def Kmeans(X, num_clusters=8, max_iter=300):\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "#    kmeans = KMeans(n_clusters=num_clusters, max_iter=max_iter).fit(X)\n",
    "    # YOUR CODE ENDS HERE\n",
    "#    labels= kmeans.labels_\n",
    "#    centers=kmeans.cluster_centers_\n",
    "#    return (labels, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans(np.array([[2,1], [2,3], [8,1], [8,3]]), num_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the synopsis (represented as TfIdf vectors) using Kmeans. You can play with different number of centers and maximum iterations to get different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time (labels, centers) = Kmeans(tfidf_matrix.todense(), num_clusters=3, max_iter=1000)\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_matrix.todense().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dataframe `frame` that stores the the clusters labels, names and genres for all the 100 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "films = {'title': titles, 'synopsis': synopsis, 'cluster': labels, 'genre': genres}\n",
    "frame = pd.DataFrame(films, index = [labels] , columns = ['title', 'cluster', 'genre'])\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie counts for a particular cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "We sort the cluster centers to get the most important terms per cluster and store it in `cluster_names`. We print them along with the movies in that cluster and look how well has KMeans worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0 words: met, reason, investig, ensu, tear, prevent,\n",
      "Cluster 0 titles: Singin' in the Rain, Amadeus, Rocky, An American in Paris, The Good, the Bad and the Ugly, Butch Cassidy and the Sundance Kid, The Apartment, High Noon, The Deer Hunter, All Quiet on the Western Front, It Happened One Night, Midnight Cowboy, Rain Man, Tootsie, Nashville, Pulp Fiction, Yankee Doodle Dandy,\n",
      "\n",
      "Cluster 1 words: pictur, intent, middl, hour, fli, afterward,\n",
      "Cluster 1 titles: The Godfather, The Shawshank Redemption, Schindler's List, Casablanca, One Flew Over the Cuckoo's Nest, Citizen Kane, The Godfather: Part II, Psycho, Sunset Blvd., The Sound of Music, West Side Story, The Silence of the Lambs, The Bridge on the River Kwai, It's a Wonderful Life, Some Like It Hot, Gandhi, From Here to Eternity, Unforgiven, A Streetcar Named Desire, To Kill a Mockingbird, Doctor Zhivago, The Pianist, Goodfellas, The French Connection, Good Will Hunting, Terms of Endearment, Fargo, The Grapes of Wrath, Shane, The Graduate, American Graffiti, Stagecoach, The Maltese Falcon, A Clockwork Orange, Taxi Driver, Wuthering Heights, Double Indemnity, Rebel Without a Cause, Rear Window, The Third Man, North by Northwest,\n",
      "\n",
      "Cluster 2 words: driver, crimin, crime, idea, pack, someon,\n",
      "Cluster 2 titles: Raging Bull, Gone with the Wind, The Wizard of Oz, Titanic, Lawrence of Arabia, Vertigo, On the Waterfront, Forrest Gump, Star Wars, E.T. the Extra-Terrestrial, 2001: A Space Odyssey, Chinatown, 12 Angry Men, Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, Apocalypse Now, The Lord of the Rings: The Return of the King, Gladiator, Saving Private Ryan, Raiders of the Lost Ark, The Philadelphia Story, The Best Years of Our Lives, My Fair Lady, Ben-Hur, Patton, Jaws, Braveheart, The Treasure of the Sierra Madre, Platoon, Dances with Wolves, The Exorcist, City Lights, The King's Speech, A Place in the Sun, Mr. Smith Goes to Washington, Annie Hall, Out of Africa, Giant, The Green Mile, Close Encounters of the Third Kind, Network, The African Queen, Mutiny on the Bounty,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = np.asarray(centers).argsort()[:, ::-1]\n",
    "cluster_names = []\n",
    "for i in range(order_centroids.shape[0]):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    q = \"\"\n",
    "    for ind in order_centroids[i][0][:6]:\n",
    "        print(' %s' % terms[ind], end=',')\n",
    "        q += str(terms[ind])\n",
    "        q += \" \"\n",
    "    cluster_names.append(q)\n",
    "    print()\n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.loc[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Data\n",
    "Now we plot the various movie clusters.\n",
    "Basically we scale the multi-dimentional feature vector by applying 2 dimensional PCA. It is a technique used to visualize multi-dimensional plots in 2 dimensions. More about it [here](http://www.apnorton.com/blog/2016/12/19/Visualizing-Multidimensional-Data-in-Python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA as sklearnPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = sklearnPCA(n_components=2) #2-dimensional PCA\n",
    "transformed = pd.DataFrame(pca.fit_transform(tfidf_matrix.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'yellow', 'black', 'gray', 'orange', 'brown']\n",
    "for i in range(len(cluster_names)):\n",
    "    plt.scatter(transformed[labels == i][0], transformed[labels == i][1], label=cluster_names[i], c=colors[i])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And you're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
